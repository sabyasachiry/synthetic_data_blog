# Scaling Machine Learning Systems with Synthetic Data #


- Wouldn't it be great to have access to large pool of quality data to increase organizational competitiveness and innovations using AI?

- What if you can improve utility of data by sharing it with  trusted business partners and governement without privacy concerns while improving collaboration and building trust?

- What if you monitize on new products and services by synthesizing real time customer data without compromising privacy information? 

- The answer to questions above is Synthetic Data, a method to statistically generate artificial data using simulations to meet specific organizational needs while improving data utility for improved data parivacy and protection.

<br>
</br>
<br align="center">
  <img width=100% src="gettyimages-107758168-2048x2048_flipped.jpg" alt="Prism Image">
</br>
<br>
</br>

# Technology trend, 2022

The benefits of implementing AI in finance is monumnetal. From natural language understanding to chatbots,  and intelligence automation to fraud detection has helped streamline processes and therefore vastly improve customer experience. By 2023, a potential cost saving of [$447 billion](https://www.businessinsider.com/ai-in-banking-report?r=US&IR=T) is estimated in banking alone from AI applications.

The benefits to Financial Services is increasingly underpinned by technological advances in AI and Machine Learning, big data and cloud infrastructure. According to the [Global Survey on AI in Financial Services](https://www.jbs.cam.ac.uk/faculty-research/centres/alternative-finance/publications/transforming-paradigms/), 77% of respondents believe AI will be of high or very high overall relevance to their organisations playing key role in revenue creation by customer support and client acquisitions, risk management, process automation, and product development. In all deployment use cases, the respondents noted that the Machine Learning and AI's biggest impact will yield from greater model accuracy and scalability.

Despite the growth of AI, there are considerable barriers to overcome. Quality of data to train model is seen as the biggest hurdle in AI implementation in addition to sub issues including data collection and data imbalance. Data confidentiality and data privacy is also mildly perceived as causing bottleneck situation to AI implementation limiting utility of data. Often organisations have to put in extra resources on time consuming activities such as data cleaning. Nearly [83% of respondents](https://www.jbs.cam.ac.uk/faculty-research/centres/alternative-finance/publications/transforming-paradigms/) believe that regulation around cross-organisational data sharing of personal information between jurisdiction to be significnatly problematic for AI implementation. 

The above has hilighted urgency in need to optimize data utility and share data using innovative privacy enhancing technologies to prevent data sharing concerns. 

## What is synthetic data?

**Synthetic data** is created artificially, by mimicking mathematical properties such as statistical distribution, probability distribution of the real-world data. It is a form of data augmentation practice to generate balanced datasets. Synthetic data is used across a wide range of industries including financial services, government,  pharma, medical, and media and entertainment. The concept of augmenting data is widely used across a variety of industries to imitate real-world scenarios. In ML/AI context it has an important use case in training, validating, evaluating ML applications, for scaling up new services and products with speed and accuracy. 

Synthetic data is a privacy-enhancing technology and can be seen as a potential solution to financial services strugglig with privacy compliance and data access. A [study published by Gartner](https://www.gartner.com/en/doc/100-data-and-analytics-predictions-through-2025) predicts that by 2024, 60% of the data used for the development of AI and analytics solutions will be synthetically generated. Gartner is ranking Synthetic data as a [forward-looking privacy technology](https://www.gartner.com/document/3987903). MIT Technology Review describes Synthetic data as one of the [10 Breakthrough Technologies, 2022](https://www.technologyreview.com/2022/02/23/1045416/10-breakthrough-technologies-2022/#synthetic-data-for-ai).

<br>
<div align="center">
  <img width="700" src="./synthetic_diagram.jpg" alt="Synthetic data"><br></br>
  <figcaption> figure1: Synthetic data use: Optimising data utility</figcaption>
</div>
</br>

## Machine learning applications and reproducibility

The accuracy and performance matrices generated by ML models depend heavily on statistical relationship between the input data. Often methods used in data aggregation coupled with sampling size and random noise can distort this relationship between the variables resulting in poor performance, which may erode trust in processes.

A [research by Gartner](https://www.gartner.com/en/newsroom/press-releases/2018-02-13-gartner-says-nearly-half-of-cios-are-planning-to-deploy-artificial-intelligence) estimates that 85% of algorithms that are currently being used are erroneous due to inherent biases within datasets. Synthetic data in addition to replica dataset can help mitigate bias and imbalance in dataset.


Reproducibility is yet an important principle in scientific method used for validation process. An outcome of an ML model is reliable if one can fully reproduce the exact result. This is critical in order to develop algorithms to reliably solve complex tasks at scale, with limited or no human supervision [2]. In research intensive industries use of synthetic data has helped researchers not only in dealing with issues around sampling size and data imbalance but also improve the reproducibility and predictive performance of AI models by ensuring complete sepeartion of training and evaluation datasets [3].

For example in cyber security,  Intrusion Detection Systems(IDS) is widely used to quickly detect anomalies in computer network and produce timely response to a potential cyber attack. ML/AI algorithms are used at the core of such system. The algorithm uses multiclass classification and clustering techniques, to find anomalies in attacker's behaviour and differenciate unusual pattern from the normal one. Such data on past attacks is rarely shared outside organisational boundries due to privacy and reputational concerns. Therefore, lack of training data makes it nearlly impossible to train IDS such that it detects unforeseen future events and edge-cases. Furthermore, an algorithm trained on such statistically imbalanced dataset often risk generating higher false negatives.


A research experiement in IDS shows that synthetic data samples generated using sophisticated technique such as Conditional Generative Adversarial Network (CTGAN) can balance training samples, using both actuals and a wide range of simulations, can increase prediction accuracy by as much as 8% [4].


## External Sharing

Financial data - such as consumer transaction records, account payments, claims and underwritings is highly sensitive and subject to stringent data protection obligations and data privacy laws [5]. Synthetic data on the otherhand masks out sensitive or personal information about real people while preserving statistical relations. An important feature for financial services, where confidentiality of client data is extremely delicate, and leaks could lead to major legal and financial issues. For example, in 2014, a data breach at JP Morgan impacted 76 million users and was estimated to cost $100 million in fines for the company. 

_To add privacy preserving features_

Organisation wide implementation of effective synthetic data policies can help generate artificial data from original data without disclosing information outside the organisation. Then the data can be shared with the trusted business partners, and third party vendors for effective customer service. In financial services benefits of sharing is not limited to relying on third party credit report or analysing market sentiments but also in effective implementation of risk management framework to speed up client onboarding process while complying with KYC/AML regulations.

The Synthetic data can be shared internally and externally without much concern.

**Operational Benefits:**

* External, Secondary data

* Vendor and client assessment

* Software testing

* Training and retention data

* Cross organizational sharing

* Synthetic data properties

* Advantages

## How can you generate synthetic data?

[Model generation diagrams](https://www.statice.ai/post/how-generate-synthetic-data)

Data augmentation is often used for creating balanced data to train, validate and evaluate models covering a wide range of use-cases and unforeseen events. For instance,  Convolutional Neural Networks (CNN) or Generative Adversarial Networks (GANs) can be utilised to produce augmented synthetic images and videos. Whereas in structured data sequences can be generated dynamically using causation and correlation patterns found in original dataset(Das A., 2020), (Dilmegani C., 2021).

Synthetic environments for scenario planning is an increasingly popular choice in reinforcement learning applications. Synthetic data generated using [Reingforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) can simulate scenario that was never seen before. For instance toolkit such as OpenAI Gym provide simulation environments to model unprecedented scenarios and edge-cases in both single and multi-agent context (Gulli, Kapoor & Pal, 2019).

### Variation Autoencoder (VAE):

VAE is belongs to a family of generative models which works with both supervised and unsupervised data. It uses probability to describe data distribution in latent space. We input real data into the Encoder which outputs data in form of vector belonging to a latent distribution. This vector is then fed to Decoder which reconstructs the data that was originally fed to the encoder. The system learns to optimises correlation between input and output of the data using iterativive optimisation process. 

![AutoEncoder](./encoder_decoder.jpg)

### GAN

Generative Adverserial Networks(GAN) belongs to a family of deep learning models and is used to generate synthetic data, including tabular data, images, and sound/speech data.

GANs are used to generate synthetic data that mimics real data. This model includes a training process that involves pitting two neural networks, a Generator and a Discriminator, against each other. The Generator is used for generating synthetic data and the Discriminator's role is to distinguish between real and synthetic data. This is done in an iterative manner such that output of the overall model improves with number of iterations. In the paper ["Data Synthesis based on Generative Adversarial Networks"](http://www.vldb.org/pvldb/vol11/p1071-park.pdf), researchers have proposed table-GAN to synthesise tabular data by preserving statistical properties of the original data while minimizing information leakage. ML/AI models thereafter can be trained using synthetic data. 

When training a model with Synthetic data, often trade-off exists between the data privacy (information leakage) and the model accuracy. In other words, a higher privacy level leads to lower degree of model compatibility as statistical attributes of the synthetic data differ greatly when privacy levels are set high. This is an ever evolving areas of research but so far it has shown promising results in generating synthetic data. 

![GAN](./GAN.png)


### Divide & Conquer 

![Divide & Conquer](./devide_and_conquer.png)


[Generating Synthetic data 1](https://research.aimultiple.com/synthetic-data-generation/)

[Generating synthetic data 2](https://research.aimultiple.com/synthetic-data-generation/)


## Use cases

## Insurance Usecase
**Insurance companies** are compelled to make most out of data they collect from users as a result of digitisation, intense competition and a new set of emerging risks. However, their ability to do so is constrained by strict privacy concerns, regulatory compliance, and legacy technology. Furthermore, regulatory developments such as [proposal to regulate AI systems](https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-laying-down-harmonised-rules-artificial-intelligence) ,released by European Commission require fast adoptation from insurance players. Synthetic data can help insurers address some of their data processing challenges.

* **Product Development**: Identify, develop and tests customer-facing products and services at speed while complying with the strictest privacy framework.
* **Sales**: By enabling secure exchange of real-time information across insurance companies can improvement user experience boosting customer conversion rate.
* **Underwriting**: Improve accuracy in underwriting process by modeling risks on balanced datasets with a greater control over statistical distribution.
* **Contract management**: Utility of data increases when a model is scaled up to infer large volume of data. With cloud technology this can be achieved easily while complying with data governance policies.
* **Claim management**: Quickly identify relational and behavioural patterns of users in large volume of data to strengthen fraud detection and claims management. 

"Synthetic insurance data is already being used by European insurers. In Switzerland, insurance company Die Mobiliar validated the use of synthetic churn data in the context of data privacy protection, adding a new tool to their digital transformation toolbox. Public authorities in Europe are also investing in synthetic data, like [in the U.K](https://dataingovernment.blog.gov.uk/2020/08/20/synthetic-data-unlocking-the-power-of-data-and-skills-for-machine-learning/). and Germany. And Gartner is ranking it as a [forward-looking privacy technology](https://www.gartner.com/document/3987903) for the coming years."

## Long term data retention - [**REFERENCE ONLY**]

"The finance sector has received more EU [GDPR fines](https://iapp.org/news/a/study-finance-sector-received-more-gdpr-fines-than-other-industries/#:~:text=A%20recent%20study%20published%20by,the%20processing%20of%20personal%20data.) than any other industry. Non-compliance involves not only fines settlements but also business disruptions, productivity loss, and revenue loss. 

In addition, legacy systems with proprietary formats and siloed IT infrastructures prevent data teams from quickly accessing data due to prolonged data access processes. 

Moreover, even when data is available, its quality might not be sufficient for cutting-edge applications.

To preserve the privacy of their data, financial companies can use synthetic data.

Compliance with personal data processing regulations is a guarantee for enterprises, making it a crucial asset. A bank, for example, would have to delete all personal information and financial information after a customer contract ends to comply with GDPR data retention requirements. 

By using privacy-preserving synthetic data, the enterprise would be able to run the long-term analysis on the synthetic financial data generated during the contract period, and delete the customer information as required by applicable regulations.

The largest companies in the world are starting to work with synthetic data. [Amazon is already using this](https://venturebeat.com/ai/amazon-go-uses-synthetic-data-to-train-cashierless-store-algorithms/) technology to improve customer purchase prediction. [American Express is also exploring](https://fortune.com/2020/09/03/american-express-deepfake-artificial-intelligence/) the topic. The data teams are researching synthetic data to train machine learning and improve their fraud detection algorithms"

# Conclusion






Refs:
[A BasisEvolution framework for network traffic anomaly detection](https://www.sciencedirect.com/science/article/abs/pii/S1389128618300331)

[Money Laundering Detection using Synthetic Dat](https://www.diva-portal.org/smash/get/diva2:834701/FULLTEXT01.pdf)

[A Copula based Framework for Generating Synthetic Data from Aggregated Sources](https://ieeexplore.ieee.org/abstract/document/9346329)


[2] [Reproducibility in Machine Learning for Health](https://openreview.net/pdf?id=HylgS2IpLN)

[3] [Synthetic data use: Medical trial example](https://link.springer.com/article/10.1007/s44163-021-00016-y)

[4] [Effect of balancing data using synthetic data on the performance of machine learning classifiers for Intrusion Detection in Computer Networks](https://arxiv.org/pdf/2204.00144.pdf)

[5] [Synthetic data to support financial services innovation](https://www.fca.org.uk/publication/call-for-input/synthetic-data-to-support-financial-services-innovation.pdf)

[6] [How do you generate synthetic data?](https://www.statice.ai/post/how-generate-synthetic-data)

[7] [Usecases in various industry - Reference only](https://www.dataversity.net/synthetic-data-4-use-cases-in-modern-enterprises/#)